{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9605298",
   "metadata": {},
   "source": [
    "# Bag of words (NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6befd949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fe9dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_dict = {'yelp':   'data/sentiment_analysis/yelp_labelled.txt',\n",
    "                 'amazon': 'data/sentiment_analysis/amazon_cells_labelled.txt',\n",
    "                 'imdb':   'data/sentiment_analysis/imdb_labelled.txt'}\n",
    "df_list = []\n",
    "for source, filepath in filepath_dict.items():\n",
    "    df = pd.read_csv(filepath, names=['sentence', 'label'], sep='\\t')\n",
    "    df['source'] = source  # Add another column filled with the source name\n",
    "    df_list.append(df)\n",
    "\n",
    "df = pd.concat(df_list)\n",
    "print(df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aee65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc37b1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp = df[df['source'] == 'yelp']\n",
    "df_yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f1a2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df_yelp['sentence'].values\n",
    "y = df_yelp['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbacd0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
    "    sentences, y, test_size=0.25, random_state=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2326be32",
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76008a2",
   "metadata": {},
   "source": [
    "## vectorizing the sentence (make each sentence into a vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003c051f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(sentences_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb34a02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer.transform(sentences_train)\n",
    "X_test  = vectorizer.transform(sentences_test)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4398539",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0985291",
   "metadata": {},
   "source": [
    "## logistic regression (baseline model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12816d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train,y_train)\n",
    "score = classifier.score(X_test,y_test)\n",
    "print (\"Accuracy:\",score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012abb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "for source in df['source'].unique():\n",
    "    df_source = df[df['source'] == source]\n",
    "    sentences = df_source['sentence'].values\n",
    "    y = df_source['label'].values\n",
    "\n",
    "    sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
    "        sentences, y, test_size=0.25, random_state=1000)\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(sentences_train)\n",
    "    X_train = vectorizer.transform(sentences_train)\n",
    "    X_test  = vectorizer.transform(sentences_test)\n",
    "\n",
    "    classifier = LogisticRegression()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    score = classifier.score(X_test, y_test)\n",
    "    print('Accuracy for {} data: {:.4f}'.format(source, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480c814b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0436b5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f15999d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e351021c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "input_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21711bd",
   "metadata": {},
   "source": [
    "## use a dense layer on top of a liner stack in keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dbc7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7451f77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer ='adam',\n",
    "             metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840beeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                      epochs=100,\n",
    "                      verbose=False,\n",
    "                       validation_data=(X_test, y_test),\n",
    "                        batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016312ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1dd272",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5d0307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2972879",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6097e157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend import clear_session\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1023f48",
   "metadata": {},
   "source": [
    "## Word Embedding - one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42709c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = ['London', 'Berlin', 'Berlin', 'New York', 'London']\n",
    "cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bdffa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "city_labels = encoder.fit_transform(cities)\n",
    "city_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611e822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "city_labels = city_labels.reshape((5, 1))\n",
    "city_labels\n",
    "encoder.fit_transform(city_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889657ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(sentences_train)\n",
    "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
    "X_test = tokenizer.texts_to_sequences(sentences_test)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(sentences_train[2])\n",
    "print(X_train[2])\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d27f26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "maxlen=100\n",
    "X_train= pad_sequences(X_train,padding = 'post',maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test,padding = 'post', maxlen = maxlen)\n",
    "print(X_train[2,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc41d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991fa176",
   "metadata": {},
   "source": [
    "## Keras Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b9b235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "\n",
    "embedding_dim = 50\n",
    "model1 = Sequential()\n",
    "\n",
    "model1.add(layers.Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=maxlen))\n",
    "model1.add(layers.Flatten())\n",
    "model1.add(layers.Dense(10, activation='relu'))\n",
    "model1.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model1.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89c0042",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model1.fit(X_train, y_train,\n",
    "                    epochs=20,\n",
    "                    verbose=False,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1819b92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model1.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model1.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89e1a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fb4972",
   "metadata": {},
   "source": [
    "## using pretrained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2924a59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath,encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e47ba1e",
   "metadata": {},
   "outputs": [],
   "source": [
    " embedding_dim = 50\n",
    "embedding_matrix = create_embedding_matrix(\n",
    " '/data/glove_word_embeddings/glove.6B.50d.txt',\n",
    "tokenizer.word_index, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaf647e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
    "nonzero_elements / vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3143cba",
   "metadata": {},
   "source": [
    "### put trainable = True to improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506f986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(layers.Embedding(vocab_size, embedding_dim, \n",
    "                           weights=[embedding_matrix], \n",
    "                           input_length=maxlen, \n",
    "                           trainable=True))\n",
    "model2.add(layers.GlobalMaxPool1D())\n",
    "model2.add(layers.Dense(10, activation='relu'))\n",
    "model2.add(layers.Dense(1, activation='sigmoid'))\n",
    "model2.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b45993",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model2.fit(X_train, y_train,\n",
    "                    epochs=50,\n",
    "                    verbose=False,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=10)\n",
    "loss, accuracy = model2.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model2.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9790835",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6053db99",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dc6ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c142e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    verbose=False,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=10)\n",
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f26918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_filters, kernel_size, vocab_size, embedding_dim, maxlen):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "    model.add(layers.Conv1D(num_filters, kernel_size, activation='relu'))\n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    model.add(layers.Dense(10, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5973c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = dict(num_filters=[32, 64, 128],\n",
    "                  kernel_size=[3, 5, 7],\n",
    "                  vocab_size=[5000], \n",
    "                  embedding_dim=[50],\n",
    "                  maxlen=[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bb62a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Main settings\n",
    "epochs = 20\n",
    "embedding_dim = 50\n",
    "maxlen = 100\n",
    "output_file = 'data/output.txt'\n",
    "\n",
    "# Run grid search for each source (yelp, amazon, imdb)\n",
    "for source, frame in df.groupby('source'):\n",
    "    print('Running grid search for data set :', source)\n",
    "    sentences = df['sentence'].values\n",
    "    y = df['label'].values\n",
    "\n",
    "  # Train-test split\n",
    "    sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
    "        sentences, y, test_size=0.25, random_state=1000)\n",
    "   # Tokenize words\n",
    "    tokenizer = Tokenizer(num_words=5000)\n",
    "    tokenizer.fit_on_texts(sentences_train)\n",
    "    X_train = tokenizer.texts_to_sequences(sentences_train)\n",
    "    X_test = tokenizer.texts_to_sequences(sentences_test)\n",
    "    \n",
    "    # Adding 1 because of reserved 0 index\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    # Pad sequences with zeros\n",
    "    X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "    X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "    \n",
    "    \n",
    "    # Parameter grid for grid search\n",
    "    param_grid = dict(num_filters=[32, 64, 128],\n",
    "                      kernel_size=[3, 5, 7],\n",
    "                      vocab_size=[vocab_size],\n",
    "                      embedding_dim=[embedding_dim],\n",
    "                      maxlen=[maxlen])\n",
    "    model = KerasClassifier(build_fn=create_model,\n",
    "                            epochs=epochs, batch_size=10,\n",
    "                            verbose=False)\n",
    "    grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid,\n",
    "                              cv=4, verbose=1, n_iter=5)\n",
    "    grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "     # Evaluate testing set\n",
    "    test_accuracy = grid.score(X_test, y_test)\n",
    "\n",
    "    \n",
    "    # Save and evaluate results\n",
    "    #prompt = input(f'finished {source}; write to file and proceed? [y/n]')\n",
    "    #if prompt.lower() not in {'y', 'true', 'yes'}:\n",
    "    #    break\n",
    "    #with open(output_file, 'a') as f:\n",
    "    s = ('Running {} data set\\nBest Accuracy : '\n",
    "             '{:.4f}\\n{}\\nTest Accuracy : {:.4f}\\n\\n')\n",
    "    output_string = s.format(\n",
    "            source,\n",
    "            grid_result.best_score_,\n",
    "            grid_result.best_params_,\n",
    "            test_accuracy)\n",
    "    print(output_string)\n",
    "        #f.write(output_string)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
